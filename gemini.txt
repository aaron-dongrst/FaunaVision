This is a comprehensive project design document for "FaunaVision". Share this directly with your team. It outlines the architecture, the specific technical strategy, and the implementation plan for each role.Project Summary: FaunaVision AI-Powered Zoo Surveillance & Welfare MonitorObjective: Build an automated surveillance pipeline that monitors zoo animals via video, classifies their behavior as Healthy (1) or Unhealthy (0), and uses Generative AI (Gemini) to explain the diagnosis to zookeepers.The Core Logic:We are not just looking for "sick" movements (which are hard to find data for). We are monitoring "Time Budgets."Healthy (1): The animal switches between behaviors (Sleeping $\rightarrow$ Eating $\rightarrow$ Walking) within normal time limits.Unhealthy (0): The animal gets stuck in one behavior loop (e.g., Pacing for >2 hours or Sleeping for >12 hours).1. System ArchitectureThe system follows a linear 4-stage pipeline:The Eye (Vision AI): Watches video and outputs specific action labels (e.g., "Walking," "Grooming") every second.The Brain (Logic Engine): Tracks the duration of these actions. It applies biological thresholds (e.g., "Rabbits shouldn't sleep for 6 hours straight").The Vet (GenAI): If "Unhealthy" is triggered, it queries a medical database and asks Gemini to generate a report.The Dashboard (UI): Displays the live video, the current status (Green/Red), and the Gemini advice.2. Role Definitions & TasksAssign these roles to your 4 team members. Each person can work independently on their file.Person A: The Vision Engineer (Backend/Model)Goal: Get the pre-trained Animal Kingdom model running locally.Input: A raw video file (.mp4).Output: A CSV file containing timestamps and confidence scores for actions.Key Tools: Python, PyTorch, SlowFast (Model Architecture).Task:Download pre-trained weights (do not train from scratch).Write a script that frames the video and passes it to the model.Map the model's 140 outputs down to 5 simple classes: Moving, Eating, Resting, Grooming, Interaction.Person B: The Logic Engineer (Data Science)Goal: Turn the CSV data into a "Healthy/Unhealthy" status.Input: The CSV file from Person A.Output: A Status Code (0 or 1) and a Reason string.Key Tools: Python (Pandas, NumPy).Task:Define the "Threshold Dictionary" (e.g., Max_Pacing_Time = 30 mins).Write the algorithm that detects consecutive labels.Logic Check: If Label == 'Moving' for 1800 consecutive frames (30s video @ 60fps) $\rightarrow$ Trigger Alert.Person C: The AI Integration Specialist (LLM)Goal: Connect the logic alert to Google Gemini for a readable report.Input: Status Code & Reason (e.g., "Status 0: Excessive Pacing").Output: A paragraph explaining why pacing is bad and what to do.Key Tools: Google Gemini API, Python.Task:Create a simple text file (vet_guidelines.txt) containing care info for the specific animal (RAG Lite).Write a prompt that injects the "Reason" and the "Text File" into Gemini.Return the text response.Person D: The Full Stack Developer (Dashboard)Goal: Bring it all together in a web app.Input: The processed video and the Gemini text.Output: A clean, interactive website running on localhost.Key Tools: Streamlit (Python).Task:Build the UI layout (Video player on left, Stats on right).Write the main.py script that calls Person A, B, and C's functions in order.3. The Official File StructureEveryone must use this structure to ensure code merges easily.Plaintextfauna-vision/
│
├── data/
│   ├── raw_videos/                # (Person D) Drop .mp4 files here
│   ├── processed_logs/            # (Person A) Where the model saves .csv files
│   └── knowledge_base/            # (Person C) 'rabbit_care_manual.txt'
│
├── models/
│   └── weights/                   # (Person A) Place .pth model files here
│
├── src/
│   ├── __init__.py
│   ├── vision.py                  # (Person A's code)
│   ├── logic.py                   # (Person B's code)
│   └── intelligence.py            # (Person C's code)
│
├── dashboard/
│   └── app.py                     # (Person D's code) - The Streamlit Interface
│
├── config.yaml                    # Global settings (API Keys, Thresholds)
├── main.py                        # Script to run the backend pipeline only
└── requirements.txt               # List of libraries to install
4. Implementation Guide (How to Start)Step 1: Environment Setup (Everyone)Run this in your terminal to ensure everyone has the same tools:Bashpip install numpy pandas streamlit google-generativeai torch torchvision opencv-python
Step 2: Downloading the "Animal Kingdom" Model (Person A)You do not need the dataset. You need the Model Zoo.Go to the Animal Kingdom GitHub.Navigate to Action Recognition.Look for I3D or SlowFast pre-trained weights.Note: If getting the academic code to run is too hard, Person A should pivot to using "HuggingFace Video Classification" (Search for MCG-NJU/videomae-base). It is much easier to implement than raw academic repositories.Step 3: Defining the Thresholds (Person B)Create the logic for the "Healthy vs. Unhealthy" decision.Python# Example Config Logic
THRESHOLDS = {
    "rabbit": {
        "sleeping_max_min": 240,  # 4 hours
        "pacing_max_min": 20,     # 20 mins (High stress)
    },
    "macaque": {
        "sleeping_max_min": 120,
        "pacing_max_min": 45,
    }
}
Step 4: The Gemini Prompt (Person C)Do not just ask Gemini "Is this bad?" Give it context.System Prompt: "You are an expert Zoo Veterinarian. You will be given a detected behavior anomaly. Use the provided Knowledge Base to explain the medical risks and suggest an enrichment activity."User Prompt: "Animal: Rabbit. Issue: Continuous Pacing for 25 minutes."5. Critical Resources & LinksThe Vision Model:Option A (Advanced): Animal Kingdom GitHubOption B (Easier): HuggingFace VideoMAE (Use this if Animal Kingdom is too hard to install).The Dashboard:Streamlit Documentation (Very easy to learn).The Intelligence:Google AI Studio (Get your Gemini API Key here).6. The "Hack" for downloading DataDo NOT download the full dataset.Go to YouTube.Search for "Rabbit Pacing" (Unhealthy) and "Rabbit Grooming" (Healthy).Download 2 clips of each using a YouTube Downloader.Use these 4 videos as your entire "Dataset" for the demo. The pre-trained model will already know what "Grooming" looks like; you just need to test if your Time Logic catches the pacing.